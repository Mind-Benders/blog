<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Mind Benders</title><link>https://mind-benders.github.io/blog/post/</link><description>Recent content in Posts on Mind Benders</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 06 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mind-benders.github.io/blog/post/index.xml" rel="self" type="application/rss+xml"/><item><title>This Month in AI - May 2023</title><link>https://mind-benders.github.io/blog/p/tmai-may-2023/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-may-2023/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-may-2023/Poster.png" alt="Featured image of post This Month in AI - May 2023" />&lt;h2 id="googles-palm-2-revolutionizing-language-modeling-with-multilingual-proficiency-reasoning-abilities-and-coding-proficiency-1">Google&amp;rsquo;s PaLM 2: Revolutionizing Language Modeling with Multilingual Proficiency, Reasoning Abilities, and Coding Proficiency. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Discover the cutting-edge advancements in AI from Google as they unveil PaLM 2, their next-generation language model. PaLM 2 has undergone extensive training on multilingual text, enabling it to understand, generate, and translate nuanced language across more than 100 languages. With improved reasoning capabilities and proficiency in programming languages, PaLM 2 demonstrates its potential for logic, common sense reasoning, mathematics, and coding tasks.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-may-2023/med-palm-dc4977b.png"
width="700"
height="339"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-may-2023/med-palm-dc4977b_hud14acc56a853a89f76597bb86a4fe8f0_46972_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-may-2023/med-palm-dc4977b_hud14acc56a853a89f76597bb86a4fe8f0_46972_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="495px"
>
&lt;/p>
&lt;p>Google has already integrated PaLM 2 into over 25 products and features, empowering users worldwide with enhanced language generation, efficient workspace features, and productivity tools. Through the development of PaLM 2, Google continues to drive innovation in AI and deliver real-world benefits in areas like healthcare and creative endeavors.&lt;/p>
&lt;h2 id="metas-ai-breakthrough-speech-recognition-for-1000-languages-now-open-source-paving-the-way-for-language-preservation-and-universal-communication-2">Meta&amp;rsquo;s AI breakthrough: Speech recognition for 1,000+ languages now open source, paving the way for language preservation and universal communication. &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Meta&amp;rsquo;s AI breakthrough empowers speech recognition in over 1,000 languages, a significant leap towards preserving endangered languages. The company is sharing these models as open source on GitHub, enabling developers to build inclusive speech applications for diverse languages. Existing speech recognition models cover a mere fraction of the world&amp;rsquo;s 7,000 languages due to limited labeled training data. Meta overcame this challenge by retraining their AI model to learn speech patterns from audio, requiring minimal data. Their models can converse in over 1,000 languages and recognize more than 4,000, with half the error rate compared to rival models. While there are risks of mistranscription and biased words, Meta&amp;rsquo;s advancements have far-reaching implications for language preservation and global communication.&lt;/p>
&lt;h2 id="ai-revolutionizes-antibiotic-discovery-unveiling-a-breakthrough-against-hospital-superbugs-3">AI Revolutionizes Antibiotic Discovery: Unveiling a Breakthrough Against Hospital Superbugs. &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Scientists from McMaster University have utilised artificial intelligence (AI) to uncover a breakthrough antibiotic called abaucin, which shows promising potential in combating drug-resistant infections, particularly Acinetobacter baumannii. This bacterium poses a significant threat in hospitals and is known to cause severe conditions like pneumonia and meningitis.&lt;/p>
&lt;p>The traditional methods of antibiotic discovery have proven challenging and time-consuming. However, AI algorithms allowed researchers to swiftly assess millions of molecules, leading to the identification of abaucin. Unlike broad-spectrum antibiotics, abaucin specifically targets A. baumannii, reducing the risk of drug resistance development and opening doors to more precise and effective treatments. This study underscores the immense potential of AI in revolutionising antibiotic discovery and providing hope in the fight against deadly hospital superbugs.&lt;/p>
&lt;h2 id="unleash-your-creativity-photoshops-ai-transforms-your-images-with-a-single-text-prompt-4">Unleash Your Creativity: Photoshop&amp;rsquo;s AI Transforms Your Images with a Single Text Prompt. &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-may-2023/photoshop.png"
width="990"
height="743"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-may-2023/photoshop_huabfc6da6be595ba42a5172869a4c5143_267598_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-may-2023/photoshop_huabfc6da6be595ba42a5172869a4c5143_267598_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>
&lt;/p>
&lt;p>Adobe has introduced a groundbreaking AI tool called Generative Fill in its Photoshop application, leveraging generative AI to add or remove objects from photos based on a simple text prompt. Acting as an &amp;ldquo;AI co-pilot,&amp;rdquo; Adobe Firefly powers this feature, aiming to revolutionize the photo editing process. While enhancing user creativity, Adobe acknowledges the need to address concerns about potential misuse of the technology. The addition of Generative Fill is expected to usher in a new era of AI-driven creativity in the creative industries, providing extraordinary results and streamlining previously time-consuming tasks. Currently available in the beta version, a wider release of this transformative AI tool in Photoshop is on the horizon.&lt;/p>
&lt;h2 id="safeguarding-the-digital-frontier-exploring-the-reality-of-ai-in-cybersecurity-5">Safeguarding the Digital Frontier: Exploring the Reality of AI in Cybersecurity. &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>In the realm of cybersecurity, the long-awaited promise of artificial intelligence (AI) is becoming a reality. AI-driven capabilities have evolved from simple rule-based systems to sophisticated tools that leverage generative AI and contextualise vast amounts of data. This breakthrough empowers cybersecurity teams with game-changing speed and accuracy, providing them with a much-needed advantage in their ongoing battle against cybercriminals. With a skills shortage and data explosion posing challenges, matured AI capabilities address these obstacles by automating tasks, improving defence postures, and enabling precise actions.&lt;/p>
&lt;p>By combining AI with automation, security teams can achieve reliable speed and enhance their ability to detect, investigate, and respond to threats. The integration of AI into threat detection and response technologies, such as IBM&amp;rsquo;s QRadar Suite, amplifies the effectiveness of security operations centres (SOCs) and streamlines the incident lifecycle. With AI&amp;rsquo;s assistance, SOC teams can prioritise real threats amidst the noise, accelerate investigation and response processes, and significantly enhance overall resilience and readiness in the cybersecurity industry.&lt;/p>
&lt;h2 id="spotifys-potential-ai-breakthrough-ai-generated-podcast-ads-6">Spotify&amp;rsquo;s Potential AI Breakthrough: AI-Generated Podcast Ads. &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>In a recent podcast episode, Bill Simmons shared that Spotify is reportedly working on AI technology that would allow podcast hosts to create host-read ads without having to personally record them. This development could offer podcasters exciting opportunities, including the creation of geo-targeted and multilingual ads, while saving valuable time for content creation. Although Spotify has not officially confirmed these claims, their ongoing investment in AI technology, exemplified by the introduction of AI DJ, suggests the possibility.&lt;/p>
&lt;p>The advent of AI-generated podcast ads would revolutionise the industry, offering podcasters a time-saving alternative and the potential to reach a broader audience. However, concerns regarding authenticity and the risk of misinformation should also be considered. The development of AI-generated podcast ads marks a significant milestone in podcasting, with the future implementation and audience response eagerly anticipated.&lt;/p>
&lt;h2 id="chegg-vs-chatgpt-the-battle-for-ai-powered-education-dominance-7">Chegg vs. ChatGPT: The Battle for AI-Powered Education Dominance. &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Chegg, an online education company, and its encounter with the disruptive force of generative AI, particularly OpenAI&amp;rsquo;s ChatGPT. Chegg&amp;rsquo;s executives had previously considered the potential of AI to replace human instructors and reduce costs but underestimated the rapid pace at which consumers embraced tools like ChatGPT.&lt;/p>
&lt;p>Initially, Chegg didn&amp;rsquo;t view ChatGPT as a threat to its paid services. However, when OpenAI released GPT-4, students began opting for ChatGPT instead of Chegg&amp;rsquo;s paid offerings, leading to a significant loss in subscriber growth and a decline in the company&amp;rsquo;s market value.&lt;/p>
&lt;p>Despite their efforts, Chegg&amp;rsquo;s future remains uncertain, and the company&amp;rsquo;s executives are focused on navigating the challenges posed by generative AI to stay relevant in the education industry.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/" target="_blank" rel="noopener"
>https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2023/05/22/1073471/metas-new-ai-models-can-recognize-and-produce-speech-for-more-than-1000-languages/" target="_blank" rel="noopener"
>https://www.technologyreview.com/2023/05/22/1073471/metas-new-ai-models-can-recognize-and-produce-speech-for-more-than-1000-languages/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://www.theguardian.com/technology/2023/may/25/artificial-intelligence-antibiotic-deadly-superbug-hospital" target="_blank" rel="noopener"
>https://www.theguardian.com/technology/2023/may/25/artificial-intelligence-antibiotic-deadly-superbug-hospital&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Future-of-Creative-Cloud-with-Generative-AI-as-a-Creative-Co-Pilot-in-Photoshop-default.aspx/default.aspx" target="_blank" rel="noopener"
>https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Future-of-Creative-Cloud-with-Generative-AI-as-a-Creative-Co-Pilot-in-Photoshop-default.aspx/default.aspx&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2023/05/24/1073395/ai-in-cybersecurity-yesterdays-promise-todays-reality/" target="_blank" rel="noopener"
>https://www.technologyreview.com/2023/05/24/1073395/ai-in-cybersecurity-yesterdays-promise-todays-reality/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://techcrunch.com/2023/05/23/spotify-may-use-ai-to-make-host-read-podcast-ads-that-sound-like-real-people/" target="_blank" rel="noopener"
>https://techcrunch.com/2023/05/23/spotify-may-use-ai-to-make-host-read-podcast-ads-that-sound-like-real-people/&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>&lt;a class="link" href="https://www.wired.com/story/chegg-embraced-ai-chatgpt-ate-its-lunch-anyway/" target="_blank" rel="noopener"
>https://www.wired.com/story/chegg-embraced-ai-chatgpt-ate-its-lunch-anyway/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>This Month in AI - April 2023</title><link>https://mind-benders.github.io/blog/p/tmai-april-2023/</link><pubDate>Sat, 06 May 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-april-2023/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-april-2023/Poster.png" alt="Featured image of post This Month in AI - April 2023" />&lt;h2 id="unlock-the-power-of-bingai-experience-the-future-today-1">Unlock the Power of BingAI: Experience the Future, Today! &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Microsoft has made Bing AI accessible to the public, eliminating the waitlist requirement. Users can now try out the AI bot by signing in to Bing using their Microsoft account on the Edge browser. The latest update introduces several exciting features powered by OpenAI&amp;rsquo;s technologies.&lt;/p>
&lt;p>Also, Bing AI now supports rich &amp;ldquo;visual answers,&amp;rdquo; displaying graphs, charts, and formatted content. The Bing Image Creator has been upgraded to support over 100 languages, enabling the generation of AI images based on text prompts and visual examples. Additionally, users can export and share chats, benefit from improved summarization capabilities for long documents, and enjoy actions within Edge for quicker access to relevant content. Microsoft is also developing third-party plug-ins to expand functionality within Bing Chat.&lt;/p>
&lt;h2 id="linkedins-intelligent-assistance-craft-the-perfect-job-application-2">LinkedIn&amp;rsquo;s Intelligent Assistance: Craft the Perfect Job Application &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>LinkedIn is reportedly testing an AI-powered feature that provides personalized writing suggestions for job seekers, aiming to help them create tailored job applications. The feature generates short cover letter-like messages using information from the user&amp;rsquo;s profile, the hiring manager&amp;rsquo;s profile, the job description, and the targeted company. While the AI-generated drafts serve as a starting point, LinkedIn emphasizes the importance of customization and encourages users to review and edit the suggestions to reflect their own voice and style.&lt;/p>
&lt;p>This development builds upon LinkedIn&amp;rsquo;s existing AI writing tool for profile creation. The adoption of AI in job application drafting reflects the growing interest in artificial intelligence, with its potential to enhance user experiences and improve outcomes in various industries, including recruitment and career development.&lt;/p>
&lt;h2 id="yolo-nas-revolutionizing-object-detection-with-unprecedented-precision-3">YOLO-NAS: Revolutionizing Object Detection with Unprecedented Precision &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Deci AI, a deep learning firm, has unveiled YOLO-NAS, its latest deep learning model designed for real-time object detection with remarkable performance. Built on Deci&amp;rsquo;s Neural Architecture Search Technology, AutoNAC™, YOLO-NAS outperforms other models like YOLOv6, YOLOv7, and YOLOv8, including the recently launched YOLOv8. AutoNAC democratizes Neural Architecture Search, enabling organizations to create customized, fast, accurate, and efficient deep learning models quickly.&lt;/p>
&lt;p>YOLO-NAS delivers superior throughput, achieving 50% more throughput and 1 mAP higher accuracy compared to other YOLO models. It is pre-trained on popular datasets, making it suitable for various real-world applications. The open-source model is available with pre-trained weights for non-commercial research use on Deci&amp;rsquo;s PyTorch-based computer vision training library called SuperGradients.&lt;/p>
&lt;h2 id="introducing-starcoder-free-code-generating-assistant-4">Introducing StarCoder: Free Code-Generating Assistant &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Hugging Face and ServiceNow Research have jointly released StarCoder, a free code-generating model that offers an alternative to existing AI systems like GitHub&amp;rsquo;s Copilot. StarCoder, part of the BigCode project, was trained on over 80 programming languages and integrates with Microsoft&amp;rsquo;s Visual Studio Code editor. Unlike other commercial models, StarCoder is royalty-free and available for use by anyone, including corporations.&lt;/p>
&lt;p>The project aims to develop state-of-the-art AI systems for code generation in an open and responsible manner. StarCoder&amp;rsquo;s release comes amidst debates around the use of public source code and licensing agreements for training AI models, with efforts made to address privacy concerns and adhere to ethical best practices.&lt;/p>
&lt;h2 id="geoffrey-hinton-ais-threat-could-be-more-urgent-than-climate-change-5">Geoffrey Hinton: AI&amp;rsquo;s Threat Could Be &amp;lsquo;More Urgent&amp;rsquo; Than Climate Change &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>In a recent interview, renowned AI pioneer Geoffrey Hinton expressed his concerns that the threat posed by artificial intelligence (AI) to humanity could be even more urgent than climate change. Hinton, often referred to as one of the &amp;ldquo;godfathers of AI,&amp;rdquo; believes that the risks associated with AI technology are significant and warrant immediate attention. Having recently left Alphabet, Hinton intends to speak out about these risks without any constraints from his former employer. As the debate around AI&amp;rsquo;s impact on society continues to unfold, Hinton&amp;rsquo;s remarks highlight the need for careful consideration and proactive measures to ensure the responsible and ethical development and deployment of AI technologies.&lt;/p>
&lt;h2 id="accelerating-the-quest-for-new-metals-ml-offers-a-promising-solution-6">Accelerating the Quest for New Metals: ML Offers a Promising Solution &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Machine learning could help develop new types of metals with useful properties, such as resistance to extreme temperatures and rust, according to new research. This could be useful in a range of sectors—for example, metals that perform well at lower temperatures could improve spacecraft, while metals that resist corrosion could be used for boats and submarines.
Usually they start off with one well-known element, like iron, which is cheap and malleable, and add one or two others to see the effect on the original material. It’s a laborious process of trial and error that inevitably yields more duds than useful results.
Researchers from the Max Planck Institute managed to identify 17 promising new metals using this method.&lt;/p>
&lt;h2 id="revolutionizing-ml-researchers-unveil-a-more-agile-approach-7">Revolutionizing ML: Researchers Unveil a More Agile Approach &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Artificial intelligence researchers have celebrated a string of successes with neural networks, computer programs that roughly mimic how our brains are organized. In 2020, two researchers at the MIT led a team that introduced a new kind of neural network based on real-life intelligence — but not our own. After a breakthrough last year, the novel networks may now be versatile enough to supplant their traditional counterparts for certain applications.&lt;/p>
&lt;p>Liquid neural networks offer an elegant and compact alternative , said Ken Goldberg, a roboticist at the University of California, Berkeley. These networks can run faster and more accurately than other so-called continuous-time neural networks, which model systems that vary over time&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://blogs.microsoft.com/blog/2023/02/22/the-new-bing-preview-experience-arrives-on-bing-and-edge-mobile-apps-introducing-bing-now-in-skype/" target="_blank" rel="noopener"
>https://blogs.microsoft.com/blog/2023/02/22/the-new-bing-preview-experience-arrives-on-bing-and-edge-mobile-apps-introducing-bing-now-in-skype/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://www.linkedin.com/pulse/how-ai-can-help-you-land-your-dream-job-step-by-step-hemachandran/" target="_blank" rel="noopener"
>https://www.linkedin.com/pulse/how-ai-can-help-you-land-your-dream-job-step-by-step-hemachandran/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://deci.ai/blog/yolo-nas-object-detection-foundation-model/" target="_blank" rel="noopener"
>https://deci.ai/blog/yolo-nas-object-detection-foundation-model/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/amp/" target="_blank" rel="noopener"
>https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/amp/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://theconversation.com/ai-pioneer-geoffrey-hinton-says-ai-is-a-new-form-of-intelligence-unlike-our-own-have-we-been-getting-it-wrong-this-whole-time-204911" target="_blank" rel="noopener"
>https://theconversation.com/ai-pioneer-geoffrey-hinton-says-ai-is-a-new-form-of-intelligence-unlike-our-own-have-we-been-getting-it-wrong-this-whole-time-204911&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2022/10/25/1062104/machine-learning-new-metals" target="_blank" rel="noopener"
>https://www.technologyreview.com/2022/10/25/1062104/machine-learning-new-metals&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>&lt;a class="link" href="https://www.quantamagazine.org/researchers-discover-a-more-flexible-approach-to-machine-learning-20230207" target="_blank" rel="noopener"
>https://www.quantamagazine.org/researchers-discover-a-more-flexible-approach-to-machine-learning-20230207&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Deep Learning Hackathon - April 2023</title><link>https://mind-benders.github.io/blog/p/deep-learning-hackathon-april-2023/</link><pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/deep-learning-hackathon-april-2023/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/deep-learning-hackathon-april-2023/dl_hackathon.png" alt="Featured image of post Deep Learning Hackathon - April 2023" />&lt;p>We are pleased to announce that we are conducting a Hackathon on Deep Learning. The Hackathon is mainly for students studying at TCET. This is a virtual hackathon conducted on GCR.&lt;/p>
&lt;p>The Hackathon will start on 13th April, 23 at sharp 05:30 PM and submissions should be done by 09:30 AM of 17th April, 23, giving you ample time to train and test your model.&lt;/p>
&lt;p>Certificates to Winners and participants will be given out.&lt;/p></description></item><item><title>Unlock the Power of Data: EDA and Automated Techniques Workshop</title><link>https://mind-benders.github.io/blog/p/unlock-the-power-of-data-eda-and-automated-techniques-workshop/</link><pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/unlock-the-power-of-data-eda-and-automated-techniques-workshop/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/unlock-the-power-of-data-eda-and-automated-techniques-workshop/eda_workshop.png" alt="Featured image of post Unlock the Power of Data: EDA and Automated Techniques Workshop" />&lt;p>We are pleased to announce that we are conducting a workshop on Unlock the Power of Data: EDA and Automated Techniques. This workshop will help students to get familiar with EDA and automated techniques involved in EDA.&lt;/p>
&lt;p>Exploratory Data Analysis (EDA) is an approach to analyze the data using visual techniques. It is used to discover trends, patterns, or to check assumptions with the help of statistical summary and graphical representations.&lt;/p>
&lt;p>We are excited to have Mr. Hrishikesh Yadav as the Speaker for this workshop. He is a Kaggle Expert, VCP @ GDSC-TCET &amp;amp; Streamlit Student Ambassador.&lt;/p>
&lt;p>The Workshop is scheduled to be conducted on 14&lt;sup>th&lt;/sup> April, 2023 from 05:00 PM onwards via Zoom.&lt;/p></description></item><item><title>This Month in AI - March 2023</title><link>https://mind-benders.github.io/blog/p/tmai-march-2023/</link><pubDate>Sun, 09 Apr 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-march-2023/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-march-2023/Poster.png" alt="Featured image of post This Month in AI - March 2023" />&lt;h2 id="breaking-language-barriers-one-word-at-a-time-with-usm---the-ultimate-speech-ai-for-over-100-languages-1">Breaking language barriers, one word at a time with USM - the ultimate speech AI for over 100 languages. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Google&amp;rsquo;s Universal Speech Model (USM) is a significant milestone towards achieving the company&amp;rsquo;s 1,000 Languages Initiative, which aims to support the world&amp;rsquo;s most spoken languages. USM is a family of state-of-the-art speech models trained on a vast dataset of 12 million hours of speech and 28 billion sentences of text, covering over 300 languages. What&amp;rsquo;s impressive about USM is that it can recognize under-resourced languages like Amharic, Cebuano, Assamese, and Azerbaijani, among others, that are spoken by fewer than twenty million people. The key to USM&amp;rsquo;s success is utilizing a large multilingual dataset to pre-train the encoder of the model and fine-tuning on a smaller set of labeled data. This approach allows the model to recognize under-represented languages and adapt to new languages and data effectively. With USM, Google is breaking language barriers and bringing greater inclusion to billions of people around the globe.&lt;/p>
&lt;h2 id="performing-real-world-navigation-googles-performer-mpc-combines-transformers-and-model-predictive-control-for-safe-and-efficient-robot-navigation-2">Performing Real-World Navigation: Google&amp;rsquo;s Performer-MPC Combines Transformers and Model Predictive Control for Safe and Efficient Robot Navigation. &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Despite decades of research, robot navigation in human-centric environments remains a challenge. Google Research is exploring how advances in machine learning, specifically Transformers models, can enable robots to navigate through tight spaces while complying with social norms. However, the deployment of massive Transformer-based controllers on mobile robots can be challenging due to strict latency constraints. To address this issue, Google Research and Everyday Robots presented Performer-MPC, an end-to-end learnable robotic system that combines a differentiable model predictive controller, Transformer-based encodings, and scalable low-rank implicit-attention Transformers with linear space and time complexity attention modules. The result is an efficient on-robot deployment that allows robots to navigate tight spaces while demonstrating socially acceptable behaviors. The research presents a significant step towards enabling safe and efficient robot navigation in real-world environments.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-march-2023/navigation.jpg"
width="1999"
height="1217"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-march-2023/navigation_hu3e7010175255d1a9f9164039a81f0acc_254454_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-march-2023/navigation_hu3e7010175255d1a9f9164039a81f0acc_254454_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="AI robots trained to navigate challenging environments and interact with humans in socially compliant ways."
class="gallery-image"
data-flex-grow="164"
data-flex-basis="394px"
>
&lt;/p>
&lt;h2 id="baidus-ernie-bot-showcases-multimodal-output-feature-setting-high-standards-for-language-models-in-china-3">Baidu&amp;rsquo;s Ernie Bot Showcases Multimodal Output Feature, Setting High Standards for Language Models in China. &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Baidu, the Chinese search engine giant, has unveiled its latest large language model (LLM), called Ernie Bot, which features a multimodal output capability. Ernie Bot, which stands for &amp;ldquo;Enhanced Representation from kNowledge IntEgration,&amp;rdquo; can solve math problems, write marketing copy, answer questions about Chinese literature, and generate multimedia responses. According to Baidu&amp;rsquo;s CEO, Robin Li, Ernie Bot performs particularly well on tasks specific to Chinese culture, such as explaining historical facts or writing traditional poems.&lt;/p>
&lt;p>The highlight of Ernie Bot&amp;rsquo;s release is its multimodal output feature, which ChatGPT and OpenAI&amp;rsquo;s recently announced GPT-4 do not offer. Ernie Bot can generate illustrations, read out text answers, and even edit and subtitle videos based on text inputs. However, there have been reports that some of the features showcased during the product launch, specifically the video generation, have failed to be reproduced in later testing. Despite this setback, Baidu&amp;rsquo;s Ernie Bot is an exciting addition to the growing list of large language models and highlights the increasing emphasis on multimodal capabilities in AI language models.&lt;/p>
&lt;h2 id="gpt-4-4">GPT-4. &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>We&amp;rsquo;ve technically already seen the release of GPT-4 more than 5 weeks ago. OpenAI says GPT-4 is 82% less likely to respond to requests for disallowed content and 40% more likely to produce factual responses than GPT-3.5 on our internal evaluations. GPT-4’s capability is impressive, from being able to produce the code for a hand-sketched website, to creating content that passed the completely human check of GPT Zero, and confusing OpenAI’s LLM text classifier. Unfortunately, due to security and competition reasons, OpenAI stated in their GPT-4 technical report that it contains no further details about the architecture , hardware, training compute, dataset construction, training method, or similar.GPT-4’s text-generation speed is significantly slower than the current free version of ChatGPT.&lt;/p>
&lt;h2 id="maximizing-collaboration-and-productivity-with-innovative-chat-plugins-5">Maximizing Collaboration and Productivity with Innovative Chat Plugins. &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Chat plugins are computer programmes that enhance the usefulness of chat programmes by adding new features and functionalities. Some possible concepts for fresh chat plugins include translation, calendar reminders, polling, task management, screen sharing, voice and video calling, social media integration, personalization, and a knowledge base. Integrating additional tools and apps with the chat application is also made possible through the integration plugin. These plugins can help users collaborate more effectively and smoothly across various platforms.&lt;/p>
&lt;h2 id="vid2seq-a-powerful-visual-language-model-for-generating-video-descriptions-6">Vid2Seq: A Powerful Visual Language Model for Generating Video Descriptions. &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Videos are an integral part of our daily lives and understanding their content has become increasingly important. Dense video captioning is a task that involves temporally localizing and describing all events in a minutes-long video. Current dense video captioning systems have several limitations such as being trained exclusively on manually annotated datasets and containing specialized task-specific components. However, a new architecture called &amp;ldquo;Vid2Seq&amp;rdquo; has been introduced which pre-trains a unified model by leveraging unlabeled narrated videos. This model improves the state of the art on a variety of dense video captioning benchmarks and also generalizes well to other related tasks. The code for Vid2Seq has also been made available for others to use.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-march-2023/vid2seq.gif"
width="778"
height="344"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-march-2023/vid2seq_hue9993907c33822ec33defe74a9f8db1c_835751_480x0_resize_box_1.gif 480w, https://mind-benders.github.io/blog/blog/p/tmai-march-2023/vid2seq_hue9993907c33822ec33defe74a9f8db1c_835751_1024x0_resize_box_1.gif 1024w"
loading="lazy"
alt="Vid2Seq: A New Visual Language Model for Dense Video Captioning with Large-Scale Pretraining."
class="gallery-image"
data-flex-grow="226"
data-flex-basis="542px"
>
&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2023/03/performer-mpc-navigation-via-real-time.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2023/03/performer-mpc-navigation-via-real-time.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2023/03/16/1069919/baidu-ernie-bot-chatgpt-launch/" target="_blank" rel="noopener"
>https://www.technologyreview.com/2023/03/16/1069919/baidu-ernie-bot-chatgpt-launch/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://techstory.in/every-ai-product-launched-in-march-2023/" target="_blank" rel="noopener"
>https://techstory.in/every-ai-product-launched-in-march-2023/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://platform.openai.com/docs/plugins/introduction" target="_blank" rel="noopener"
>https://platform.openai.com/docs/plugins/introduction&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Kaggle Experts - Notebooks &amp; Discussion Workshop</title><link>https://mind-benders.github.io/blog/p/kaggle-experts-notebooks-discussion-workshop/</link><pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/kaggle-experts-notebooks-discussion-workshop/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/kaggle-experts-notebooks-discussion-workshop/kaggle_note.png" alt="Featured image of post Kaggle Experts - Notebooks &amp; Discussion Workshop" />&lt;p>We are pleased to announce that we are conducting a workshop on Kaggle Notebooks &amp;amp; Discussion. This our 3rd Installment in Kaggle Series to become Kaggle Expert.&lt;/p>
&lt;p>Kaggle Notebooks are computational environment that enables reproducible and collaborative analysis to run ML Code. Designing interactive Notebooks that showcase best of your skills is must to become a Kaggle Expert.&lt;/p>
&lt;p>We are excited to have Mr. Somesh Fengade as the Speaker for this workshop. He is a Data science intern at Culinda and Kaggle Expert. He is also an AWS AI &amp;amp; ML Scholarship Recipient.&lt;/p>
&lt;p>The Workshop is scheduled to be conducted on 02&lt;sup>nd&lt;/sup> April, 2023 from 05:00 PM onwards via Zoom.&lt;/p></description></item><item><title>This Month in AI - February 2023</title><link>https://mind-benders.github.io/blog/p/tmai-february-2023/</link><pubDate>Sun, 05 Mar 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-february-2023/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-february-2023/Poster.png" alt="Featured image of post This Month in AI - February 2023" />&lt;h2 id="beyond-directions-the-exciting-evolution-of-maps-towards-immersive-and-sustainable-solutions-1">Beyond Directions: The Exciting Evolution of Maps towards Immersive and Sustainable Solutions. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Google Maps has launched a new feature called &amp;ldquo;Immersive view&amp;rdquo; that lets users explore a place virtually and get a feel of what it&amp;rsquo;s like before visiting. It uses AI and computer vision to create a rich, digital model of the world and provides helpful information like weather, traffic, and how busy a place is. The feature is rolling out in major cities and will soon launch in more cities.&lt;/p>
&lt;p align="center">
&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/arwn_indoor_framed_02.gif" alt="Navigate airports, train stations, and shopping centers with indoor Live View" />
&lt;/p>
&lt;p>This new update is part of Google Maps&amp;rsquo; vision for an immersive and sustainable navigation experience that reimagines how people explore and navigate while helping them make more informed choices. With this feature, users can plan their visits with more accurate and detailed information and explore nearby restaurants to get a sense of their vibe before booking a reservation.&lt;/p>
&lt;h2 id="revolutionizing-cancer-research-how-ai-is-helping-detect-genes-that-fuel-cancer-growth-2">Revolutionizing Cancer Research: How AI is Helping Detect Genes that Fuel Cancer Growth. &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Artificial intelligence (AI) technology has been used to identify genes critical to cancer cell survival, which could aid in the development of personalized cancer treatments. The University of Sussex researchers developed an algorithm that analyses genetic changes in tumors to identify essential genes, and using this technology, actionable targets could be identified to personalize treatments. Currently, cancer treatments are prescribed based on cancer type and location, but personalized treatment could improve life expectancy and reduce side effects. Targeting protein products of tumor-specific genes can kill cancer cells while leaving normal cells unharmed.&lt;/p>
&lt;h2 id="bard-unleashing-the-power-of-ai-language-models-to-expand-your-knowledge-and-imagination-3">Bard: Unleashing the Power of AI-Language Models to Expand Your Knowledge and Imagination. &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Google CEO Sundar Pichai highlights the importance of AI in unlocking human potential and improving lives. He introduces Bard, an experimental conversational AI service powered by Google&amp;rsquo;s Language Model for Dialogue Applications (LaMDA), which draws information from the web to provide high-quality responses. Pichai also discusses Google&amp;rsquo;s latest AI advancements, including PaLM, Imagen, and MusicLM, and their potential to create new ways of engaging with information. Google plans to introduce AI-powered features in Search that will distill complex information and multiple perspectives into easy-to-digest formats. Additionally, Google will onboard developers to try their Generative Language API and provide a suite of tools and APIs to build more innovative AI applications. Pichai emphasizes Google&amp;rsquo;s commitment to developing AI responsibly and making it safe and useful for everyone.&lt;/p>
&lt;p>
&lt;img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/AI_features_feb6.gif"
loading="lazy"
alt="When looking for insights, AI features in Search can distill information to help you see the big picture."
>
&lt;/p>
&lt;h2 id="the-making-of-chatgpt-an-inside-look-from-the-team-behind-the-ai-language-model-4">The Making of ChatGPT: An Inside Look from the Team behind the AI-Language Model. &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>In late November 2022, OpenAI launched ChatGPT with little fanfare, intending it as a &amp;ldquo;research preview&amp;rdquo; of a two-year-old technology that needed public feedback to polish it. However, the chatbot became a viral sensation and one of the most popular internet apps ever, catching OpenAI off guard. Since then, the company has been working hard to capitalize on its success and push the technology forward by updating ChatGPT several times. OpenAI uses adversarial training to prevent users from tricking ChatGPT into behaving badly. OpenAI has also signed a multibillion-dollar deal with Microsoft and an alliance with Bain, which plans to use OpenAI&amp;rsquo;s generative AI models in marketing campaigns for its clients, including Coca-Cola. The buzz around ChatGPT has led to a gold rush around large language models, with companies and investors worldwide getting into the action.&lt;/p>
&lt;h2 id="exploring-the-potential-of-artificial-intelligence-in-addressing-global-challenges-5">Exploring the Potential of Artificial Intelligence in Addressing Global Challenges. &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>A study has investigated the potential of artificial intelligence to address societal mega-trends and analyzed its proposed solutions in dealing with these global challenges. Artificial intelligence can offer understandable insights into the complex and cross-cutting issues of mega-trends, and how they could change and benefit different areas if AI systems are deployed. a more powerful version of the currently popular ChatGPT chatbot, to analyze the potential of AI for societal megatrends.&lt;/p>
&lt;p>These are major global issues such as digitization, urbanization, globalization, climate change, automation, mobility, global health issues, aging population, emerging markets, and sustainability. The study concluded that AI can significantly improve understanding of these megatrends by providing insights into how they might evolve over time and what solutions might be implemented.&lt;/p>
&lt;h2 id="gen-1-transforming-videos-with-text-and-image-editing-in-generative-ai-6">Gen-1: Transforming Videos with Text and Image Editing in Generative AI. &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>The new model presented in this work is a structure and content-guided video diffusion model that allows users to edit videos based on visual or textual descriptions of the desired output. The model overcomes conflicts between user-provided content edits and structure representations, which have previously arisen due to the insufficient disentanglement of the two aspects. The authors propose a solution to this problem by training the model on monocular depth estimates with varying levels of detail, which provides control over structure and content fidelity.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-february-2023/gen-1.png"
width="512"
height="512"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-february-2023/gen-1_hue44d8e5aa53ce58b021bee5e8289f611_125242_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-february-2023/gen-1_hue44d8e5aa53ce58b021bee5e8289f611_125242_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="The model is trained jointly on images and videos, which also allows for explicit control of temporal consistency through a novel guidance method."
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>
&lt;/p>
&lt;p>The model is trained jointly on images and videos, which also allows for explicit control of temporal consistency through a novel guidance method. The authors demonstrate the success of their model through various experiments, including fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards the results produced by the model.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://blog.google/products/maps/sustainable-immersive-maps-announcements/" target="_blank" rel="noopener"
>https://blog.google/products/maps/sustainable-immersive-maps-announcements/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://economictimes.indiatimes.com/magazines/panache/ai-can-be-leveraged-to-detect-genes-that-drive-growth-of-cancer-cells/articleshow/97657463.cms" target="_blank" rel="noopener"
>https://economictimes.indiatimes.com/magazines/panache/ai-can-be-leveraged-to-detect-genes-that-drive-growth-of-cancer-cells/articleshow/97657463.cms&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://blog.google/technology/ai/bard-google-ai-search-updates/" target="_blank" rel="noopener"
>https://blog.google/technology/ai/bard-google-ai-search-updates/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/" target="_blank" rel="noopener"
>https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://www.sciencedaily.com/releases/2023/02/230227132653.htm" target="_blank" rel="noopener"
>https://www.sciencedaily.com/releases/2023/02/230227132653.htm&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://research.runwayml.com/gen1" target="_blank" rel="noopener"
>https://research.runwayml.com/gen1&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>This Month in AI - January 2023</title><link>https://mind-benders.github.io/blog/p/tmai-january-2023/</link><pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-january-2023/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-january-2023/Poster.png" alt="Featured image of post This Month in AI - January 2023" />&lt;h2 id="ai-and-big-data-will-be-the-most-impactful-emerging-technologies-for-pharma-industry-in-2023-1">AI And Big Data Will Be The Most Impactful Emerging Technologies For Pharma Industry In 2023. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>GlobalData’s latest report, ‘The State of the Biopharmaceutical Industry – 2023’, reveals that AI and Big Data were trending as the two most disruptive emerging technologies since 2020, with a significant margin from the third choice in all four years.&lt;/p>
&lt;p>Elton Kwok, Market Research Manager in Pharma at GlobalData, comments: Drug-developing and other processes are complex and highly structured in the pharmaceutical industry. The processes generate a vast amount of data, especially in the current digital age; however, this data can be useless or meaningless if it is not properly analyzed. AI not only can help process the data more efficiently, saving time and labor costs; it can also produce analysis more accurately since it feeds on high-quality data, which comes from proprietary datasets.&lt;/p>
&lt;h2 id="chatgpt-developer-closes-in-on-30bn-valuation-in-talks-to-raise-capital-2">ChatGPT developer closes in on $30bn valuation in talks to raise capital. &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>OpenAI, the artificial intelligence research lab behind chatbot ChatGPT, is in talks to sell existing shares in a tender offer that would value the company at about $29 billion, the Wall Street Journal reported, citing people familiar with the matter.OpenAI&amp;rsquo;s chatbot is a software application designed to mimic human-like conversation based on user prompts and can respond to a large range of questions while imitating human speaking styles. The firm expects business to surge as it pitched to investors saying the organization expects $200 million in revenue next year and $1 billion by 2024, Reuters reported in December.&lt;/p>
&lt;h2 id="turning-text-into-sound-ai-revolutionizing-the-rhythm-of-the-music-industry-3">Turning text into sound: AI revolutionizing the rhythm of the music industry. &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Google AI has introduced MusicLM, a cutting-edge machine learning model that generates high-quality music from text descriptions. This innovative technology casts the process of music generation as a hierarchical sequence-to-sequence task, resulting in music that is both consistent and of a high audio quality.&lt;/p>
&lt;p>MusicLM outperforms existing systems in both audio quality and accuracy to the given text description. It can even be conditioned on both text and melody, meaning that it can transform whistled or hummed melodies into a specific style described in the text caption.&lt;/p>
&lt;p>To support future research, Google AI has released MusicCaps, a dataset of 5.5k music-text pairs, with text descriptions written by human experts. This dataset is sure to be a valuable resource for music researchers and enthusiasts alike.&lt;/p>
&lt;h2 id="empowering-home-assistants-with-human-like-interaction-the-next-step-in-ai-evolution-4">Empowering Home Assistants with Human-Like Interaction: The Next Step in AI Evolution. &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-january-2023/home_assistant.jpg"
width="800"
height="360"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-january-2023/home_assistant_hu098608bfd138db166a692cce29f61163_36752_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-january-2023/home_assistant_hu098608bfd138db166a692cce29f61163_36752_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Empowering Home Assistants with Human-Like Interaction: The Next Step in AI Evolution"
class="gallery-image"
data-flex-grow="222"
data-flex-basis="533px"
>
&lt;/p>
&lt;p>MIT researchers have developed the NOPA framework, a system that allows AI agents to autonomously determine the best way to assist human users at different times. NOPA enables the creation of more responsive and socially intelligent robots and home assistants by simultaneously inferring what task the human user is trying to tackle and appropriately assisting them without waiting for explicit instructions from the human. The framework uses a neural network to continuously predict a set of possible goals for the human user, which are then evaluated and updated. The AI assistant uses inverse planning to search for common subgoals and specific actions that can help humanity achieve its goals. The researchers evaluated NOPA in a simulated environment and found that agents were able to adjust their behavior to minimize disruption, such as putting back objects they picked if they were not related to the task.&lt;/p>
&lt;h2 id="buzzfeed-boosts-business-with-openai-embracing-the-future-of-ai-assisted-content-creation-5">BuzzFeed Boosts Business with OpenAI: Embracing the Future of AI-Assisted Content Creation. &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>BuzzFeed&amp;rsquo;s shares rose by 120% bringing the company&amp;rsquo;s market value to $300 million after it announced plans to use OpenAI to enhance its content creation, including its quizzes. OpenAI&amp;rsquo;s publicly available API will be used by BuzzFeed and this move toward AI-assisted content comes as OpenAI&amp;rsquo;s viral chatbot, ChatGPT, has gained popularity and sparked a debate over AI&amp;rsquo;s role in the workplace. Analysts at Cowen note that AI can lower the cost of content but the timing and impact of its adoption is unclear. BuzzFeed is also reportedly working with creators to produce content for Facebook&amp;rsquo;s parent company, Meta Platforms, in a deal valued at almost $10 million.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://aithority.com/machine-learning/ai-and-big-data-will-be-most-impactful-emerging-technologies-for-pharma-industry-in-2023/" target="_blank" rel="noopener"
>https://aithority.com/machine-learning/ai-and-big-data-will-be-most-impactful-emerging-technologies-for-pharma-industry-in-2023/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://www.weforum.org/agenda/2023/01/4-things-you-need-to-know-about-ai-january-2023/" target="_blank" rel="noopener"
>https://www.weforum.org/agenda/2023/01/4-things-you-need-to-know-about-ai-january-2023/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://google-research.github.io/seanet/musiclm/examples/" target="_blank" rel="noopener"
>https://google-research.github.io/seanet/musiclm/examples/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://techxplore.com/news/2023-01-framework-social-intelligence-home.html" target="_blank" rel="noopener"
>https://techxplore.com/news/2023-01-framework-social-intelligence-home.html&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://nypost.com/2023/01/27/buzzfeed-stock-surges-on-plan-to-use-chatgpt-parent-openai-to-create-online-content/" target="_blank" rel="noopener"
>https://nypost.com/2023/01/27/buzzfeed-stock-surges-on-plan-to-use-chatgpt-parent-openai-to-create-online-content/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>This Month in AI - December 2022</title><link>https://mind-benders.github.io/blog/p/tmai-december-2022/</link><pubDate>Thu, 05 Jan 2023 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-december-2022/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-december-2022/Poster.png" alt="Featured image of post This Month in AI - December 2022" />&lt;h2 id="chatgpt-the-latest-ai-based-language-model-developed-by-openai-1">ChatGPT: The latest AI-based language model developed by openai. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>ChatGPT is a variant of the GPT (Generative Pre-training Transformer) language model designed to handle longer-form text, specifically text written in characters instead of words. This makes it particularly well-suited for translation, machine writing, and language modeling tasks in languages that do not use spaces between words.&lt;/p>
&lt;p>One of the key benefits of ChatGPT is its ability to handle out-of-vocabulary (OOV) words more effectively than traditional word-based language models. In a word-based model, OOV words can be a significant problem because they are not part of the model&amp;rsquo;s vocabulary and therefore difficult to process. ChatGPT, on the other hand, can handle OOV words by treating them as a sequence of individual characters, allowing it to generate more accurate and coherent text.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/ChatGPT.jpg"
width="780"
height="440"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/ChatGPT_hud9a7993ec29288671d1195c40b8f4867_25352_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-december-2022/ChatGPT_hud9a7993ec29288671d1195c40b8f4867_25352_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="ChatGPT"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="425px"
>
&lt;/p>
&lt;p>Another advantage of ChatGPT is its ability to handle languages with a high degree of morphological complexity, such as inflection and word concatenation. In these languages, a single word can have multiple meanings depending on its inflection, and word boundaries may not always be clearly defined. ChatGPT is able to handle this complexity by processing text at the character level, allowing it to generate more accurate and natural-sounding text in these languages.&lt;/p>
&lt;h2 id="differential-privacy-preserving-individual-privacy-in-a-world-of-data-sharing-2">Differential privacy: Preserving individual privacy in a world of data sharing. &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Differential privacy is a concept in the field of computer science that refers to the practice of protecting the privacy of individuals in a dataset by adding noise to the data before it is analyzed. This noise helps to obscure the data of individual users, making it more difficult to identify or profile specific individuals within the dataset.&lt;/p>
&lt;p>Differential privacy (DP) is a framework for measuring the privacy of an algorithm. It does this by quantifying the &amp;ldquo;privacy cost&amp;rdquo; of the algorithm, which refers to the amount of information about an individual that is leaked as a result of running the algorithm.&lt;/p>
&lt;p>Composition is a key property of DP that reflects the net privacy cost of a combination of DP algorithms when they are viewed as a single algorithm. An example of this is the differentially-private stochastic gradient descent (DP-SGD) algorithm, which trains machine learning models over multiple iterations, each of which is differentially private. The basic composition theorem in DP states that the privacy cost of a collection of algorithms is, at most, the sum of the privacy cost of each individual algorithm. However, this can often be a gross overestimate, and several improved composition theorems have been developed to provide better estimates of the privacy cost of composition. These theorems help to more accurately understand the privacy cost of a combination of DP algorithms and ensure that it does not exceed certain thresholds.&lt;/p>
&lt;h2 id="ehr-safe-realistic-ehr-data-for-research-and-analysis-without-compromising-patient-privacy-3">EHR-Safe: Realistic EHR data for research and analysis, without compromising patient privacy. &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>EHR-Safe is a tool for generating synthetic electronic health records (EHRs) that are both high-fidelity and privacy-preserving. It is designed to allow researchers and analysts to work with real-world EHR data without exposing the sensitive personal information of individual patients.&lt;/p>
&lt;p>One of the key features of EHR-Safe is its ability to generate synthetic EHRs that are indistinguishable from real EHRs. This is achieved through the use of advanced machine learning techniques that are trained on real EHR data. The resulting synthetic EHRs are able to capture the structure, variability, and complexity of real EHRs, making them an ideal tool for research and analysis.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/EHR-Safe.png"
width="1999"
height="392"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/EHR-Safe_hu1a68c578dbb527315c80bc9142f10626_281816_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-december-2022/EHR-Safe_hu1a68c578dbb527315c80bc9142f10626_281816_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="EHR-Safe"
class="gallery-image"
data-flex-grow="509"
data-flex-basis="1223px"
>
&lt;/p>
&lt;p>In addition to its ability to generate high-fidelity synthetic EHRs, EHR-Safe is also designed to protect the privacy of individual patients. It does this through the use of differential privacy techniques, which add noise to the data in order to obscure the data of individual patients. This helps to ensure that the sensitive personal information of individual patients is not exposed while still allowing researchers to work with real-world EHR data.&lt;/p>
&lt;h2 id="rt-1-revolutionizing-robotics-with-flexible-adaptable-and-autonomous-models-4">RT-1: Revolutionizing robotics with flexible, adaptable, and autonomous models. &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Recent advances in machine learning (ML) research, such as computer vision and natural language processing, have been enabled by the use of large, diverse datasets and expressive models. However, this approach has not yet been successfully applied to robotics due to a lack of large-scale and diverse robotic data and the lack of expressive, scalable, and fast models that can learn from this data and generalize effectively. Data collection for robotics is particularly expensive and challenging, and there is a need for more expressive and fast models that can learn from this data in real-time.&lt;/p>
&lt;p>Robotics Transformer 1 (RT1) aims to revolutionize the field of robotics by enabling the development of robots that are more flexible, adaptable, and autonomous based on the transformer architecture.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/RT-1.png"
width="1999"
height="548"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/RT-1_hu5059ceb0a4546aa6aba6424391251544_384528_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-december-2022/RT-1_hu5059ceb0a4546aa6aba6424391251544_384528_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="RT1"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="875px"
>
&lt;/p>
&lt;p>Key features of RT1:
ability to process a wide range of sensory inputs, including visual, auditory, and tactile data. This allows it to perceive and interact with its environment in a more natural and intuitive way, making it better suited for tasks such as object recognition and manipulation.
ability to adapt to changing environments and tasks. It is able to learn from experience and make adjustments to its behavior in order to better accomplish its goals. This makes it more flexible and adaptable than many traditional robotics systems, which are typically hard-coded to perform specific tasks.&lt;/p>
&lt;h2 id="cicero-empowering-human-ai-cooperation-through-negotiation-and-persuasion-5">CICERO: Empowering human-AI cooperation through negotiation and persuasion. &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>CICERO is an AI agent that has achieved human-level performance in the strategy game Diplomacy. CICERO demonstrated this by ranking in the top 10% of participants on webDiplomacy.net, an online version of the game. CICERO&amp;rsquo;s success in Diplomacy is notable because it requires players to understand other people&amp;rsquo;s motivations and perspectives, make complex plans, and use natural language to negotiate and form alliances with other players. CICERO is so effective at using natural language to negotiate in the game that other players often preferred working with it over other human participants.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/CICERO.png"
width="1920"
height="1069"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-december-2022/CICERO_hub7149bce198521a78e9f719b957f7b73_122612_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-december-2022/CICERO_hub7149bce198521a78e9f719b957f7b73_122612_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="CICERO"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="431px"
>
&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://openai.com/blog/chatgpt/" target="_blank" rel="noopener"
>https://openai.com/blog/chatgpt/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/12/ehr-safe-generating-high-fidelity-and.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/12/ehr-safe-generating-high-fidelity-and.html&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/" target="_blank" rel="noopener"
>https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Kaggle Competitions Workshop</title><link>https://mind-benders.github.io/blog/p/kaggle-competitions-workshop/</link><pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/kaggle-competitions-workshop/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/kaggle-competitions-workshop/kaggle_poster.png" alt="Featured image of post Kaggle Competitions Workshop" />&lt;p>We are pleased to announce that we are conducting a workshop on Kaggle Competetions. Kaggle is the CodeChef For Data Scientist. It offers a no-setup, customisable, Jupyter Notebooks environment.&lt;/p>
&lt;p>Kaggle Competitions are designed to provide challenges for competitors at all different stages of their machine learning careers. As a result, they are very diverse, with a range of broad types.&lt;/p>
&lt;p>We are excited to have Mr. Debarshi Chanda as the Speaker for this workshop. He is an IIT-Guwahati Undergrad and is Kaggle Notebooks Grandmaster. He is also an upcoming Applied Data Scientist @ Amazon and is Weights &amp;amp; Biases Dev Expert.&lt;/p>
&lt;p>The Workshop is scheduled to be conducted on 18&lt;sup>th&lt;/sup> December, 2022 from 05:00 PM onwards via Zoom.&lt;/p></description></item><item><title>This Month in AI - November 2022</title><link>https://mind-benders.github.io/blog/p/tmai-november-2022/</link><pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-november-2022/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-november-2022/Poster.jpg" alt="Featured image of post This Month in AI - November 2022" />&lt;h2 id="best-language-models-that-have-become-prominent-in-nlp-1">Best Language Models that have become prominent in NLP &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Recent studies revealed that Language Models(LMs) have become more prominent and increasingly important in NLP research and impactful practice. The performance of NLP tasks has been improved by scaling up language models. Scaling up LMs required a great number of computational resources.&lt;/p>
&lt;p>The researchers have explored two complementary approaches for improving existing language models which are &amp;ldquo;Transcending Scaling Laws with 0.1% Extra Compute&amp;rdquo; and &amp;ldquo;Scaling Instruction-Fine Tuned Language Models&amp;rdquo;.
In the First method, they introduced UL2R which improves performance across a range of NLP tasks in “Scaling Instruction-Finetuned Language Models”, they explore fine-tuning a language model on a collection of datasets phrased as instructions, a process they call “Flan”.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-november-2022/LMs.png"
width="640"
height="491"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-november-2022/LMs_hua06c5b0ed6b9e85567285c298e92a1dd_75146_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-november-2022/LMs_hua06c5b0ed6b9e85567285c298e92a1dd_75146_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Compute versus model performance of PaLM 540B and U-PaLM 540B on 26 NLP benchmarks (listed in Table 8 in the paper). U-PaLM 540B continues training PaLM for a very small amount of compute but provides a substantial gain in performance."
>
&lt;figcaption>Compute versus model performance of PaLM 540B and U-PaLM 540B on 26 NLP benchmarks (listed in Table 8 in the paper). U-PaLM 540B continues training PaLM for a very small amount of compute but provides a substantial gain in performance.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>Google researchers show that Flan and UL2R can be combined as a model called Flan-U-PaLM 540B. As LMs become even larger, techniques such as UL2R and Flan that improve general performance without large amounts of compute may become increasingly attractive.&lt;/p>
&lt;h2 id="in-machine-learning-the-use-of-synthetic-data-improves-the-performance-of-a-models-2">In machine learning, the use of synthetic data improves the performance of a models &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Researchers train machine learning models using vast datasets of video clips that show humans performing actions. Using these videos might also violate copyright or data protection laws.&lt;/p>
&lt;p>So, researchers have come up with a solution instead of using realistic datasets they are turning to use synthetic datasets. These are made by a computer that uses 3D models of scenes, objects, and humans to quickly produce many varying clips of specific actions and this type of data does not violate copyright.&lt;/p>
&lt;p>To test whether the synthetic data works as good as real data researchers used 150,000 synthetic datasets that captured a wide range of human actions to train machine-learning models. The researchers found that the synthetically trained models performed even better than models trained on real data for videos that have fewer background objects.&lt;/p>
&lt;h2 id="aws-machine-learning-university-is-now-providing-a-new-program-3">AWS Machine Learning University is now providing a new program &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>AWS Machine Learning University is now providing a new program that will help institutions serving historically underserved and underrepresented students deliver courses in next-gen tech with a free, comprehensive educator-enablement bootcamp and a curriculum based on the same courses Amazon uses to train its own developers and data scientists.&lt;/p>
&lt;p>Amazon Web Services (AWS) Machine Learning University is now launching a free program helping community colleges teach databases, artificial intelligence (AI), and machine learning concepts. The program combines an educator-enablement bootcamp with a rich curriculum to help institutions get course content and increase their teaching capacity. Black and Latino students earn bachelor&amp;rsquo;s degrees in engineering at a much lower rate than their white peers.&lt;/p>
&lt;p>Amazon Web Services (AWS) Machine Learning University is now launching a free program helping community colleges teach database, artificial intelligence (AI), and machine learning (ML) concepts. The program combines an educator-enablement bootcamp with a rich curriculum to help institutions get course content and increase their teaching capacity. Black and Latino students earn bachelor&amp;rsquo;s degrees in engineering at a much lower rate than their white peers. Amazon Web Services (AWS) is launching a turnkey teaching solution for educational institutions. The educator-enablement bootcamps will start January 2023, and the curriculum materials will be available in spring. HCC will be the first community college in the U.S. to offer a bachelor&amp;rsquo;s degree in AI in fall 2023 pending final approval from the Southern Association of Colleges and Schools Commission on Colleges.&lt;/p>
&lt;h2 id="ai-will-thrive-in-3-key-areas-in-2023-despite-economic-conditions-4">AI will thrive in 3 key areas in 2023, despite economic conditions &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Some of the biggest tech names have laid off artificial intelligence (AI) and machine learning employees this fall. AI experts expect AI innovation to continue, even in the midst of a possible recession. &amp;ldquo;AI will continue to be central to business by cutting costs and increasing innovation,&amp;rdquo; they say.&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-november-2022/4.jpg"
width="750"
height="375"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-november-2022/4_huc9f5cfe09785666efcf3633be0c9a0ce_174234_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-november-2022/4_huc9f5cfe09785666efcf3633be0c9a0ce_174234_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>
&lt;/p>
&lt;p>&amp;ldquo;AI won&amp;rsquo;t replace humans in the near term,&amp;rdquo; said Vishal Sikka, founder, and CEO of human-centered AI platform, Vian AI. In 2023, the recognition that too many platforms aren&amp;rsquo;t designed for humans to use will increase. More and more systems will be designed to amplify human judgment.&lt;/p>
&lt;h2 id="use-of-ai-to-better-prepare-for-climate-migration-5">Use of AI to Better prepare for climate migration &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Climate change is one of the scariest crises of our time causing natural disasters that often result in the displacement of large groups of people, known as ‘climate migrants.’ According to the United Nations International Organization for Migration, up to one billion people will become climate migrants over the next three decades. This projection rises to 1.2 billion by 2050 and 1.4 billion by 2060.&lt;/p>
&lt;p>The responsible use of AI offers a unique perspective into climate migration that can benefit climate migrants and states alike. Current data sources include national authorities, NGOs and IGOs and administrative data sources, such as humanitarian visa numbers. However, these are often updated after a disaster happens and do not reflect the urgency of the matter at hand.
The use of AI as a predictive and preventative mechanism allows individuals and governments to make the necessary preparations before a natural disaster strikes.&lt;/p>
&lt;h2 id="a-far-sighted-approach-to-machine-learning-6">A far-sighted approach to machine learning &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>A new technique enables artificial intelligence agents to think much farther into the future when considering how their behaviors can influence the behaviors of other AI agents, toward the completion of a task. This approach improves long-term performance of cooperative or competitive AI agents.&lt;/p>
&lt;p>Creating artificial intelligence agents that can learn to compete and cooperate as effectively as humans remains a thorny problem. Because of the complexity of this problem, current approaches tend to be myopic; the agents can only guess the next few moves of their teammates or competitors, which leads to poor performance in the long run.
Their machine-learning framework enables cooperative or competitive AI agents to consider what other agents will do as time approaches infinity, not just over a few next steps.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/11/better-language-models-without-massive.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/11/better-language-models-without-massive.html&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://news.mit.edu/2022/synthetic-data-ai-improvements-1103" target="_blank" rel="noopener"
>https://news.mit.edu/2022/synthetic-data-ai-improvements-1103&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://www.aboutamazon.com/news/aws/aws-launches-new-ai-program-for-community-colleges-msis-hbcus" target="_blank" rel="noopener"
>https://www.aboutamazon.com/news/aws/aws-launches-new-ai-program-for-community-colleges-msis-hbcus&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://venturebeat.com/ai/ai-will-thrive-in-3-areas-despite-economic-conditions-experts-say/" target="_blank" rel="noopener"
>https://venturebeat.com/ai/ai-will-thrive-in-3-areas-despite-economic-conditions-experts-say/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://www.weforum.org/agenda/2022/11/how-ai-can-help-climate-migration/" target="_blank" rel="noopener"
>https://www.weforum.org/agenda/2022/11/how-ai-can-help-climate-migration/&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://www.sciencedaily.com/releases/2022/11/221123114212.htm" target="_blank" rel="noopener"
>https://www.sciencedaily.com/releases/2022/11/221123114212.htm&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Git &amp; Github Workshop - Part 2</title><link>https://mind-benders.github.io/blog/p/git-github-workshop-part-2/</link><pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/git-github-workshop-part-2/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/git-github-workshop-part-2/keval_git.png" alt="Featured image of post Git &amp; Github Workshop - Part 2" />&lt;p>Learn how to start building, shipping, and maintaining software with Git &amp;amp; GitHub. Mind Benders is pleased to announce that the workshop will be conducted you Keval Waghate from TT AI&amp;amp;DS.&lt;/p>
&lt;p>The event will be conducted on 19th November, 22 from 5:00pm onwards.&lt;/p>
&lt;p>So Why Git?&lt;/p>
&lt;ul>
&lt;li>Over 70% of developers use Git!&lt;/li>
&lt;li>Developers can work together from anywhere in the world.&lt;/li>
&lt;li>Developers can see and revert to earlier versions of the project.&lt;/li>
&lt;/ul>
&lt;p>What will you learn:&lt;/p>
&lt;ol>
&lt;li>Git &amp;amp; Github&lt;/li>
&lt;li>Manage projects with Repositories&lt;/li>
&lt;li>Clone a project to work on a local copy&lt;/li>
&lt;li>Branch and Merge to work on different parts and versions of a project&lt;/li>
&lt;li>Push &amp;amp; Pull the latest version of the project to Github&lt;/li>
&lt;/ol></description></item><item><title>This Month in AI - October 2022</title><link>https://mind-benders.github.io/blog/p/tmai-october-2022/</link><pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-october-2022/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-october-2022/Poster.jpg" alt="Featured image of post This Month in AI - October 2022" />&lt;h2 id="an-ai-that-can-help-to-design-new-proteins-and-to-unleash-new-cures-and-accouterments-1">An AI that can help to design new proteins and to unleash new cures and accouterments. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>A group of experimenters from the University of Washington described a new tool ProteinMPNN, which could help experimenters discover preliminarily unknown proteins and design entirely new bones. It will open an entire new macrocosm of possible proteins for experimenters to design from scratch.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-october-2022/1.jpg"
width="807"
height="454"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-october-2022/1_hud3fa4513dfaf2a56f67a896e5d1e2b3e_37508_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-october-2022/1_hud3fa4513dfaf2a56f67a896e5d1e2b3e_37508_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="A structure of protein discovered by researchers using AI"
>
&lt;figcaption>A structure of protein discovered by researchers using AI&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>ProteinMPNN will help experimenters with the inverse problem. Machine literacy will make the whole process a lot more hastily and light, and will allow experimenters to produce fully new proteins and structures on a much larger scale.&lt;/p>
&lt;h2 id="an-open-source-unified-language-learner-2">An Open Source Unified Language Learner &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Google researchers have presented a novel language pre-training paradigm called Unified Language Learner that improves the performance of language models universally across datasets and setups. Building models that understand and generate natural language well is one of the grand goals of machine learning research. Improving the quality of our language models is a key target for researchers to make progress toward such a goal.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://raw.githubusercontent.com/Mind-Benders/blog-content/main/post/blogs/2022/october_2022/2.gif"
loading="lazy"
alt="An overview of the denoising objectives used in UL2’s mixture-of-denoisers."
>
&lt;figcaption>An overview of the denoising objectives used in UL2’s mixture-of-denoisers.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="ai-can-detect-drunkenness-by-evaluating-infrared-images-of-human-faces-with-93-accuracy-3">AI can detect drunkenness by evaluating infrared images of human faces with 93% accuracy &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Artificial intelligence has developed a way to detect whether a person is drunk or not using thermal imaging. The technology could be used by police to screen people in city centers or at events where alcohol is likely to have been consumed. Thermal imaging offers a less ambiguous approach that is also non-invasive, researchers say.&lt;/p>
&lt;p>Any system designed to identify inebriated people must have a very low rate of false positives and false negatives. After all, a false negative might see a drunk person driving their car whereas too many false positives would lead to frustration and a loss of trust in the system among the public.&lt;/p>
&lt;h2 id="an-open-source-image-database-that-unlocks-the-power-of-ai-for-ocean-exploration-4">An open-source image database that unlocks the power of AI for ocean exploration &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Fathom Net is an open-source image database that uses state-of-the-art data processing algorithms to help process the backlog of visual data. Using artificial intelligence and machine learning will alleviate the bottleneck for analyzing underwater imagery and accelerate important research around ocean health.&lt;/p>
&lt;p>Recent advances in machine learning enable fast, sophisticated analysis of visual data. Still, the use of AI in ocean research has been limited by the lack of a standard set of existing images that could be used to train the machines to recognize and catalog underwater objects and life. This will surely help in ocean cleaning.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-october-2022/4.jpg"
width="800"
height="448"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-october-2022/4_hu00bfb3346a60f9a511cde0e2b3a59990_40509_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-october-2022/4_hu00bfb3346a60f9a511cde0e2b3a59990_40509_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Labeled image data for deep learning algorithms require an annotation and localization for a single concept."
>
&lt;figcaption>Labeled image data for deep learning algorithms require an annotation and localization for a single concept.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="how-ai-image-generators-could-help-robots-5">How AI image generators could help robots &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>The field of AI-generated art can be traced back as far as the 1960s with early attempts using symbolic rule-based approaches to make technical images. The way these generative models generate images: initially, you have this really nice image, where you start from this random noise, and you basically learn how to simulate the process of how to reverse this process of going from noise back to your original image, where you try to iteratively refine this image to make it more and more realistic.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-october-2022/5.jpg"
width="800"
height="533"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-october-2022/5_hu70b0eaeb2ce12b5290f9872adc81255b_61909_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-october-2022/5_hu70b0eaeb2ce12b5290f9872adc81255b_61909_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Researchers have been working on extending stable diffusion models, the technical backbone of generative art to other domains such as robotics."
>
&lt;figcaption>Researchers have been working on extending stable diffusion models, the technical backbone of generative art to other domains such as robotics.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>The researchers describe how these models can work wrt to robotics: as you can generate different images, you can also generate different robot trajectories (the path and schedule), and by composing different models together, you are able to generate trajectories with different combinations of skills. If I have natural language specifications of jumping versus avoiding an obstacle, you could also compose these models together, and then generate robot trajectories that can both jump and avoid an obstacle.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2022/09/15/1059550/an-ai-that-can-design-new-proteins-could-help-unlock-new-cures-and-materials/" target="_blank" rel="noopener"
>https://www.technologyreview.com/2022/09/15/1059550/an-ai-that-can-design-new-proteins-could-help-unlock-new-cures-and-materials/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://techxplore.com/news/2022-10-ai-network-drunkenness-infrared-images.html" target="_blank" rel="noopener"
>https://techxplore.com/news/2022-10-ai-network-drunkenness-infrared-images.html&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://techxplore.com/news/2022-10-open-source-image-database-power-ai.html" target="_blank" rel="noopener"
>https://techxplore.com/news/2022-10-open-source-image-database-power-ai.html&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://techxplore.com/news/2022-10-ai-image-robots.html" target="_blank" rel="noopener"
>https://techxplore.com/news/2022-10-ai-image-robots.html&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Kaggle Expert Webinar</title><link>https://mind-benders.github.io/blog/p/kaggle-expert-webinar/</link><pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/kaggle-expert-webinar/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/kaggle-expert-webinar/kaggle_poster.png" alt="Featured image of post Kaggle Expert Webinar" />&lt;p>The Club is conducting a webinar on Kaggle. Kaggle is the CodeChef For Data Scientist. It offers a no-setup, customisable, Jupyter Notebooks environment.&lt;/p>
&lt;p>Inside Kaggle you’ll find all the code &amp;amp; data you need to do your data science work. Use over 50,000 public datasets and 400,000 public notebooks to conquer any analysis in no time.&lt;/p>
&lt;p>We are thrilled to announce that the Speaker for the webinar is Khushi Shah who is 3x Kaggle Expert and ML Research Intern@ Manentia AI. Se was also a Research Intern @ IIT Patna.&lt;/p>
&lt;p>The Webinar is to be conducted on 09th October, 2022 at 05:00 PM on Zoom.&lt;/p></description></item><item><title>This Month in AI - September 2022</title><link>https://mind-benders.github.io/blog/p/tmai-september-2022/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-september-2022/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-september-2022/Poster.jpg" alt="Featured image of post This Month in AI - September 2022" />&lt;h2 id="ai-reduces-a-100000-equation-quantum-physics-problem-to-4-equations-1">AI Reduces a 100,000-Equation Quantum Physics Problem to 4 Equations &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Physicists have compressed a daunting quantum problem that required 100,000 equations into just four. Breakthrough could help scientists better understand the behavior of electrons in materials. Machine learning tool could also be used in other fields, such as cosmology and neuroscience.&lt;/p>
&lt;p>Physicists have compressed a daunting quantum problem that required 100,000 equations into just four using AI. The renormalization group is a jumbo-size set of equations that must be solved. Di Sante and his colleagues used a machine learning program which captured the Hubbard model&amp;rsquo;s physics with just four equations.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/physics_Eq.png"
width="800"
height="250"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/physics_Eq_hu4d93cd80ca2964ebec1b35e4470159de_305687_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/physics_Eq_hu4d93cd80ca2964ebec1b35e4470159de_305687_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="The Flatiron Institute&amp;rsquo;s researchers used machine learning to crunch down the problem to just four equations, each representing a single interaction between electrons."
>
&lt;figcaption>The Flatiron Institute&amp;#39;s researchers used machine learning to crunch down the problem to just four equations, each representing a single interaction between electrons.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>This breakthrough could help scientists better understand the behavior of electrons in materials, which could lead to new technologies. There are also exciting possibilities for using the technique in other fields that deal with renormalization groups, such as cosmology and neuroscience.&lt;/p>
&lt;h2 id="google-lens--text-to-speech-gets-major-upgrades-2-3">Google Lens &amp;amp; Text To Speech gets major upgrades &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Google has annonced that they have updated all 421 voices in 67 languages with a new voice model and synthesizer. This update is available for all 64bit Android Devices by updating/installing the Speech Services by Google app.&lt;/p>
&lt;p>The model is built using fresher speaker data, which alongside new stack, results in a drastic improvement in speech synthesis.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Text-to-Speech.png"
width="682"
height="203"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Text-to-Speech_hu8731e07fe8b4f8a13a89f781a5a7faa3_32677_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Text-to-Speech_hu8731e07fe8b4f8a13a89f781a5a7faa3_32677_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="All 421 voices in 67 languages have been upgraded with a new voice model and synthesizer. Voice Services by Google speech engine providing clearer, more natural voices."
>
&lt;figcaption>Voice Services by Google speech engine providing clearer, more natural voices.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>Google showed off a new feature for Google Lens, which uses ML and augmented reality (ar) to deliver a more seamless real-time translation experience. The new Lens AR Translate experience doesn&amp;rsquo;t show any bars on the original image. It utilizes machine learning to erase the original text, recreate the pixels underneath with an AI-generated background, and then overlay the translated text. The feature will be available in 59 languages.&lt;/p>
&lt;p>The company claims the experience will roll out later this year. Google uses the same technology that powers the Magic Eraser feature on Pixel devices to do this.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Google-Lens.png"
width="960"
height="540"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Google-Lens_hua243391b2e6b42e82843091a683dcefd_123466_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Google-Lens_hua243391b2e6b42e82843091a683dcefd_123466_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Google Lens&amp;rsquo; real-time translations will soon look a lot more seamless."
>
&lt;figcaption>Google Lens&amp;#39; real-time translations will soon look a lot more seamless.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="lyra-v2---a-better-faster-and-more-versatile-speech-codec-4">Lyra V2 - a better, faster, and more versatile speech codec &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Lyra is a open-source speech codec that Google developed to improve the quality of speech recognition.&lt;/p>
&lt;p>Lyra V2 is faster, efficient, versatile and is designed to be used in low-power devices, such as smart speakers and mobile phones. It is based on E2E neural audio codec called SoundStream. It offers scalable bitrate capabilities, better performance, and generates higher quality audio.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Lyra-V2.png"
width="1500"
height="450"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Lyra-V2_hu2ce5e9a4d827be431362dd137227b3c6_89967_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/Lyra-V2_hu2ce5e9a4d827be431362dd137227b3c6_89967_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Lyra V2 is a speech codec that Google developed to improve the quality of speech recognition."
>
&lt;figcaption>Model of Lyra V2 Codec&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="gaudi-a-neural-architect-for-immersive-3d-scene-generation-5">GAUDI: A Neural Architect for Immersive 3D Scene Generation &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>GAUDI is a neural architecture that can generate realistic 3D scenes from a single image designed to be used in VR. It&amp;rsquo;s generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/gaudi.png"
width="238"
height="212"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/gaudi_huccca8dd1ba113af2b9cbf0748b44bcf3_7136_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/gaudi_huccca8dd1ba113af2b9cbf0748b44bcf3_7136_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="GAUDI is a neural architecture that can generate 3D scenes from a single image."
>
&lt;figcaption>Example 3D Scenes generated by GAUDI&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>GAUDI obtains state-of-the-art performance in the unconditional generative setting across multiple datasets.&lt;/p>
&lt;h2 id="metas-make-a-video-an-ai-system-that-generates-videos-from-text-6">Meta&amp;rsquo;s Make-A-Video: An AI system that generates videos from text &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Make-A-Video is a new AI system that can generate videos from text prompts. It&amp;rsquo;s designed to be used in virtual reality, and it can generate realistic 3D scenes from a single image. It&amp;rsquo;s also designed to be used in virtual reality, and it can generate realistic 3D scenes from a single image.&lt;/p>
&lt;p>The system learns what the world looks like from paired text-image data. It can also create videos from images or take existing videos and create new ones that are similar.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://raw.githubusercontent.com/Mind-Benders/blog-content/main/post/blogs/2022/september_2022/makeavideo.webp"
loading="lazy"
alt="Make-A-Video is a new AI system that can generate videos from text &amp;lsquo;Robot dancing in times square&amp;rsquo;."
>
&lt;figcaption>Video generated from text &amp;#39;Robot dancing in times square&amp;#39; using Make-A-Video&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>Make-A-Video follows Meta&amp;rsquo;s announcement earlier of &lt;a class="link" href="https://mind-benders.github.io/blog/p/tmai-july-2022/#the-meta-ai--greater-creative-control-for-ai-image-generation-1" title="Make-A-Scene"
>Make-A-Scene&lt;/a>&lt;/p>
&lt;h2 id="nvidia-dlss-3-higher-fps-using-ai-7">NVIDIA DLSS 3: Higher FPS using AI &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>NVIDIA DLSS revolutionized graphics by using AI super resolution and Tensor Cores on GeForce RTX GPUs to boost frame rates while delivering crisp, high quality images that rival native resolution.&lt;/p>
&lt;p>DLSS 3 is a revolutionary breakthrough in AI-powered graphics that massively boosts performance, while maintaining great image quality and responsiveness. Building upon DLSS Super Resolution, DLSS 3 adds Optical Multi Frame Generation to generate entirely new frames, and integrates NVIDIA Reflex low latency technology for optimal responsiveness. It can lead upto 4x performace over brute-force rendering.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/nvidia-dlss-3-working.jpg"
width="3591"
height="1803"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/nvidia-dlss-3-working_huefe76e5ed7462452e18e07fc1a5007fc_1530079_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/nvidia-dlss-3-working_huefe76e5ed7462452e18e07fc1a5007fc_1530079_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Working of NVIDIA&amp;rsquo;s DLSS 3"
>
&lt;figcaption>Working of NVIDIA&amp;#39;s DLSS 3 which improves FPS by using lower resolution input frames to output higher resolution frames&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="pytorch-foundation-a-new-era-for-the-cutting-edge-ai-framework-8">PyTorch Foundation: A new era for the cutting-edge AI framework &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>PyTorch is moving to a new, independent PyTorch Foundation, under the Linux Foundation umbrella. PyTorch was built with an open source, community-first philosophy, and that will not change. The foundation&amp;rsquo;s mission is to drive adoption of AI tooling by fostering and sustaining an ecosystem of open source projects. It will democratize state-of-the-art tools, libraries, and other components to make these innovations accessible to everyone.&lt;/p>
&lt;p>PyTorch was created by a group of Meta AI researchers to fix the tedious, complicated pipeline of the AI field. Meta will continue to invest in the framework, and use it as the primary framework for our AI research and production. The framework will continue to be a part of Meta&amp;rsquo;s AI research and engineering work.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/pytorch.jpg"
width="603"
height="344"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/pytorch_hu535661bce2c742bf7a0f6a055ac0b6d1_86398_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/pytorch_hu535661bce2c742bf7a0f6a055ac0b6d1_86398_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt=" PyTorch Foundation will focus on the business and product marketing of PyTorch and the related ecosystem"
>
&lt;figcaption>PyTorch Foundation will focus on the business and product marketing of PyTorch and the related ecosystem&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="the-eu-wants-to-put-companies-on-the-hook-for-harmful-ai-9">The EU wants to put companies on the hook for harmful AI &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>The EU is creating new rules to make it easier to sue AI companies for harm. A bill unveiled this week is likely to become law in a couple of years. The bill would give people and companies the right to sue for damages after being harmed by an AI system. It will add teeth to the EU&amp;rsquo;s AI Act, which is set to become law around the same time.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/EU.png"
width="1200"
height="800"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-september-2022/EU_hucec48095be29928448e5feedad6dc36a_32370_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-september-2022/EU_hucec48095be29928448e5feedad6dc36a_32370_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Whether or not it succeeds, this new EU legislation will have a ripple effect on how AI is regulated around the world."
>
&lt;figcaption>Whether or not it succeeds, this new EU legislation will have a ripple effect on how AI is regulated around the world.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>The EU wants to be the global gold standard for AI regulation. Other countries, such as the US, are considering similar regulations. The Federal Trade Commission is considering rules around how companies handle data and build algorithms. It has forced companies that have collected data illegally to delete their algorithms.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://www.simonsfoundation.org/2022/09/26/artificial-intelligence-reduces-a-100000-equation-quantum-physics-problem-to-only-four-equations/" target="_blank" rel="noopener"
>https://www.simonsfoundation.org/2022/09/26/artificial-intelligence-reduces-a-100000-equation-quantum-physics-problem-to-only-four-equations/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://android-developers.googleblog.com/2022/09/listen-to-our-major-text-to-speech-upgrades-for-64-bit-devices.html?m=1" target="_blank" rel="noopener"
>https://android-developers.googleblog.com/2022/09/listen-to-our-major-text-to-speech-upgrades-for-64-bit-devices.html?m=1&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://www.xda-developers.com/google-lens-ar-translate/" target="_blank" rel="noopener"
>https://www.xda-developers.com/google-lens-ar-translate/&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://opensource.googleblog.com/2022/09/lyra-v2-a-better-faster-and-more-versatile-speech-codec.html" target="_blank" rel="noopener"
>https://opensource.googleblog.com/2022/09/lyra-v2-a-better-faster-and-more-versatile-speech-codec.html&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://machinelearning.apple.com/research/gaudi" target="_blank" rel="noopener"
>https://machinelearning.apple.com/research/gaudi&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://ai.facebook.com/blog/generative-ai-text-to-video/" target="_blank" rel="noopener"
>https://ai.facebook.com/blog/generative-ai-text-to-video/&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>&lt;a class="link" href="https://www.nvidia.com/en-us/geforce/news/dlss3-ai-powered-neural-graphics-innovations/" target="_blank" rel="noopener"
>https://www.nvidia.com/en-us/geforce/news/dlss3-ai-powered-neural-graphics-innovations/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>&lt;a class="link" href="https://ai.facebook.com/blog/pytorch-foundation/" target="_blank" rel="noopener"
>https://ai.facebook.com/blog/pytorch-foundation/&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>&lt;a class="link" href="https://www.technologyreview.com/2022/10/01/1060539/eu-tech-policy-harmful-ai-liability/" target="_blank" rel="noopener"
>https://www.technologyreview.com/2022/10/01/1060539/eu-tech-policy-harmful-ai-liability/&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>This Month in AI - August 2022</title><link>https://mind-benders.github.io/blog/p/tmai-august-2022/</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-august-2022/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-august-2022/August_2022.jpg" alt="Featured image of post This Month in AI - August 2022" />&lt;h2 id="new-ai-powered-app-could-boost-smartphone-battery-life-by-30-1">New AI-Powered App Could Boost Smartphone Battery Life by 30% &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/AI_Powered_App.jpg"
width="350"
height="233"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/AI_Powered_App_huaf0d32193d947eacc75729d341eda392_14857_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-august-2022/AI_Powered_App_huaf0d32193d947eacc75729d341eda392_14857_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="The app also optimizes the performance of other apps running at the same time."
>
&lt;figcaption>The app also optimizes the performance of other apps running at the same time.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>EOptomizer App is a new cutting-edge AI invention spearheaded by Dr. Amit Singh, the University of Essex. The app could potentially increase smartphone battery life by upto 30% thus saving electricity and bills. The app uses AI to optimize performance, heat generation, and efficiency of the chip.&lt;/p>
&lt;p>The app is conceived to be used throughout the industry and help reduce carbon emissions.&lt;/p>
&lt;h2 id="highly-efficient-new-neuromorphic-chip-for-ai-on-the-edge-2">Highly-Efficient New Neuromorphic Chip for AI on the Edge &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>The NeuRRAM chip is the first compute-in-memory chip to demonstrate a wide range of AI applications while using just a small percentage of the energy consumed by other platforms while maintaining equivalent accuracy.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/NeuRRAM-Chip.jpg"
width="777"
height="518"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/NeuRRAM-Chip_hu0da248f4a70ed80d9f56fee568e6e4a4_69159_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-august-2022/NeuRRAM-Chip_hu0da248f4a70ed80d9f56fee568e6e4a4_69159_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt=" A team of international researchers designed, manufactured, and tested the NeuRRAM chip. Credit: David Baillot/University of California San Diego "
>
&lt;figcaption>A team of international researchers designed, manufactured, and tested the NeuRRAM chip. Credit: David Baillot/University of California San Diego&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>Intel has unveiled a chip that can run a wide variety of AI applications at a fraction of the energy consumed by other platforms. NeuRRAM chip can perform sophisticated cognitive tasks anywhere and anytime without relying on a network connection to a central server. It is twice as energy efficient as the state-of-the-art &amp;ldquo;compute-in-memory&amp;rdquo; chips.&lt;/p>
&lt;h2 id="van-goghs-hidden-portrait-recreated-135-years-after-being-painted-3">Van Gogh’s ‘hidden’ portrait recreated 135 years after being painted &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Portrait of two male wrestlers was discovered 10 years ago beneath another Van Gogh painting. It shows the two men locked in combat with splashes of red blood against a background of blue strokes. X-ray technology had previously revealed the painting, but artificial intelligence has now brought it to life in colour.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/135.png"
width="990"
height="732"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/135_hu727246767dc7d761c7aa8859f15f2eb5_2134319_480x0_resize_box_3.png 480w, https://mind-benders.github.io/blog/blog/p/tmai-august-2022/135_hu727246767dc7d761c7aa8859f15f2eb5_2134319_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="The Two Wrestlers recreation is set to be exhibited in The Louvre (Supplied)"
>
&lt;figcaption>The Two Wrestlers recreation is set to be exhibited in The Louvre (Supplied)&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>This painting was thought to have been lost or destroyed until it was rediscovered behind another Van Gogh work in 2012.&lt;/p>
&lt;h2 id="digitizing-smell-using-molecular-maps-to-understand-odor-4">Digitizing Smell: Using Molecular Maps to Understand Odor &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Smells are produced by molecules that waft through the air and enter our noses, and bind to sensory receptors. Potentially billions of molecules can produce a smell, so figuring out which ones produce which smells is difficult to catalog or predict. Sensory maps can help us solve this problem, and have existed for centuries, but not for smell.
The model works by exploring thousands of examples of distinct molecules paired with the smell labels that they evoke, e.g., &amp;ldquo;beefy&amp;rdquo;, &amp;ldquo;floral&amp;rdquo;, or &amp;ldquo;minty&amp;rdquo;.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/POM.jpg"
width="1999"
height="1061"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-august-2022/POM_hudb92b64d30c07def8b03d8f5c82b4090_165366_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-august-2022/POM_hudb92b64d30c07def8b03d8f5c82b4090_165366_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt=" The Principal Odor Map is a map that maps molecules chemical character. Individual molecules locations are determined by their odour, and the locations of these points reflect predictions of their odor character. "
>
&lt;figcaption>The Principal Odor Map is a map that maps molecules chemical character. Individual molecules locations are determined by their odour, and the locations of these points reflect predictions of their odor character.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://scitechdaily.com/new-ai-powered-app-could-boost-smartphone-battery-life-by-30/" target="_blank" rel="noopener"
>https://scitechdaily.com/new-ai-powered-app-could-boost-smartphone-battery-life-by-30/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://scitechdaily.com/highly-efficient-new-neuromorphic-chip-for-ai-on-the-edge/" target="_blank" rel="noopener"
>https://scitechdaily.com/highly-efficient-new-neuromorphic-chip-for-ai-on-the-edge/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://www.independent.co.uk/arts-entertainment/art/van-gogh-hidden-painting-wrestlers-b2155439.html" target="_blank" rel="noopener"
>https://www.independent.co.uk/arts-entertainment/art/van-gogh-hidden-painting-wrestlers-b2155439.html&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>MLH Fellowship Webinar</title><link>https://mind-benders.github.io/blog/p/mlh-fellowship-webinar/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/mlh-fellowship-webinar/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/mlh-fellowship-webinar/mlh_poster.jpg" alt="Featured image of post MLH Fellowship Webinar" />&lt;p>The Club is conducting a webinar on MLH Fellowship. MLH Fellowship is a fully remote, 12-week internship where participants earn a stipend and learn to collaborate on real open source projects with peers and engineers from top companies. During the MLH Fellowship, you&amp;rsquo;ll gain the type of applied experience that employers want to see.&lt;/p>
&lt;p>We are thrilled to announce that the Speaker for the webinar is Asjad Khan who is MLH Fellow'22 @ Solana Labs and MLH Prep Fellow in April'22. He was also an Outreachy'21 Intern @ Eclipse Adoptium.&lt;/p>
&lt;p>The Webinar is to be conducted on 2nd September, 2022 at 05:00 PM on Zoom.&lt;/p>
&lt;p>Register Here: &lt;a class="link" href="https://forms.gle/4ggBKuqv9opVCUbL6" target="_blank" rel="noopener"
>https://forms.gle/4ggBKuqv9opVCUbL6&lt;/a> to attend the Webinar.&lt;/p></description></item><item><title>This Month in AI - July 2022</title><link>https://mind-benders.github.io/blog/p/tmai-july-2022/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/tmai-july-2022/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/tmai-july-2022/July_2022.png" alt="Featured image of post This Month in AI - July 2022" />&lt;p>In this world of fast-paced progress, everyday we try to find out new updates, new features and new inventions that are shaping our future life like never before. Day by day, the information surrounding us increases, bombarding us with bits only to be forgotten or missed leaving us completely overwhelmed in the process .&lt;/p>
&lt;p>By collecting &amp;amp; organizing the data, we have built tools to build models and generate AI that have started to pop in the real world. Lots of people consider AI as an evolutionary change in human life, which is quite debatable. But collecting knowledge about all the new changes is still possible for us.&lt;/p>
&lt;p>Thus, The Month in AI Blog Series aims to summarize newest trends &amp;amp; research breakthroughs that will shape the world of AI.&lt;/p>
&lt;h2 id="the-meta-ai--greater-creative-control-for-ai-image-generation-1">The Meta AI : Greater creative control for AI image generation &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Meta AI published the AI research concept known as Make-A-Scene. Make-A-Scene had the potential to empower the imagination into a life.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/meta_generative_ai.jpg"
width="1920"
height="1080"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/meta_generative_ai_hube3343b77de99cb499300b8d9b324547_358389_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-july-2022/meta_generative_ai_hube3343b77de99cb499300b8d9b324547_358389_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Colorful Cat Sculpture Genereated by Make-A-Scene"
>
&lt;figcaption>Colorful Cat Sculpture Genereated by Make-A-Scene&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>It can be hard to generate compositions of two different parts. If we consider the example ‘cat hanging to the tree’. This type of prediction may be hard for the model. The height of the tree is smaller than a cat or bigger than a cat, Is the cat hanging to the left side or right side, Is it facing upside or downside. So reflection of image won&amp;rsquo;t be the solution of this perdition.&lt;/p>
&lt;p>To overcome this problem the scientist starts working on the sketch part so if a non-artist draws anything he can get the output that they are imagining.&lt;/p>
&lt;h2 id="smart-textiles-sense-how-their-users-are-moving-2">Smart textiles sense how their users are moving &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>07 July, 2022 MIT Researchers developed a comfortable, form-fitting fabric that recognizes its wearer’s activities, like walking, running, and jumping.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://news.mit.edu/sites/default/files/styles/news_article__image_gallery/public/images/202207/MIT_3D-Knits-01-PRESS_0.jpg"
loading="lazy"
alt=" a sock made from the novel fabrication process "
>
&lt;figcaption>A sock made from the novel fabrication process&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/MIT_Train_detect.jpg"
width="1280"
height="720"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/MIT_Train_detect_huaa2a1721b5436991bdfc52cf7753dcb5_100698_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-july-2022/MIT_Train_detect_huaa2a1721b5436991bdfc52cf7753dcb5_100698_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Workout Detection"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/MIT_3D-Knits-03-PRESS.jpg"
width="1669"
height="1112"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/MIT_3D-Knits-03-PRESS_huebbfad1edfa8e198fe68a27ac6a615ca_320887_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-july-2022/MIT_3D-Knits-03-PRESS_huebbfad1edfa8e198fe68a27ac6a615ca_320887_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="Composition of Materials"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>
&lt;/p>
&lt;p>Using a novel fabrication process, MIT researchers have produced smart textiles that snugly conform to the body so they can sense the wearer’s posture and motions.&lt;/p>
&lt;p>The technique could have many applications, especially in health care and rehabilitation. For example, it could be used to produce smart shoes that track the gait of someone who is learning to walk again after an injury, or socks that monitor pressure on a diabetic patient’s foot to prevent the formation of ulcers.&lt;/p>
&lt;h2 id="ml-enhanced-code-completion-improves-developer-productivity-3">ML-Enhanced Code Completion Improves Developer Productivity &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>Today we describe how we combined ML and SE to develop a novel Transformer-based hybrid semantic ML code completion, now available to internal Google developers.&lt;/p>
&lt;p>The increasing complexity of code poses a key challenge to productivity in software engineering. Code completion has been an essential tool that has helped mitigate this complexity in integrated development environments (IDEs).&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4vLV_OWhGUw9u2badD0ZlCFdl1mwBlYzQCQ_XRTtCyu2DKy-yGZhKgnRvruSrAdqQOpThDNi0lbpHjSn5b18aqJ-pJnvCZTEOgrpyVO75CWGTugrrN4e_RY65v_bUdLkOvmDBI9v2j3qA7pWaUAeorGXUzAYHShoNff52lK1lSd2u1seEOSn9JW7hvg/s1043/image5.gif"
loading="lazy"
alt="Integrated full line completions by ML continuing the semantic dropdown completion that is in focus."
>
&lt;figcaption>Integrated full line completions by ML continuing the semantic dropdown completion that is in focus.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF3-J_lvEG2eJ1Tgypq_kisxte9X2VbDAVnlS5J2b7JIV4Iv_6WJIPcBp78bPP6ZeXZuv3libP_IzsYJbUQwirfjGRWiKSAH3-6GnaKZEqSeH6_neQyGHdeI9PhItukRTeUMcVYQB9c2TOXbQd01roh-X2PZlCJM53CEgeWZeer0vA1krUnekFNHucuQ/s509/image1.png"
loading="lazy"
alt="Suggestions of multiple line completions by ML."
>
&lt;figcaption>Suggestions of multiple line completions by ML.&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>A common approach to code completion is to train transformer models, which use a self-attention mechanism for language understanding, to enable code understanding and completion predictions. We treat code similar to language, represented with sub-word tokens and a SentencePiece vocabulary, and use encoder-decoder transformer models running on TPUs to make completion predictions. The input is the code that is surrounding the cursor (~1000-2000 tokens) and the output is a set of suggestions to complete the current or multiple lines. Sequences are generated with a beam sea rch (or tree exploration) on the decoder.&lt;/p>
&lt;h2 id="dalle-2---openai-4">DALL·E 2 - OpenAI &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>The artificial intelligence research lab OpenAI revealed DALL·E 2, the successor to 2021’s DALL·E. Both AI systems can generate astounding images from natural-language text descriptions.&lt;/p>
&lt;p>DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/OpenAI_asto.jpg"
width="1024"
height="1024"
srcset="https://mind-benders.github.io/blog/blog/p/tmai-july-2022/OpenAI_asto_hue0d9d6109f0a18f37e84771b9851ce4b_113034_480x0_resize_q75_box.jpg 480w, https://mind-benders.github.io/blog/blog/p/tmai-july-2022/OpenAI_asto_hue0d9d6109f0a18f37e84771b9851ce4b_113034_1024x0_resize_q75_box.jpg 1024w"
loading="lazy"
alt="An astronaut riding a horse in a photorealistic style"
>
&lt;figcaption>An astronaut riding a horse in a photorealistic style&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/" target="_blank" rel="noopener"
>https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://news.mit.edu/2022/smart-textiles-sense-movement-0707" target="_blank" rel="noopener"
>https://news.mit.edu/2022/smart-textiles-sense-movement-0707&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html" target="_blank" rel="noopener"
>https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://openai.com/dall-e-2/" target="_blank" rel="noopener"
>https://openai.com/dall-e-2/&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Competitive Programming Workshop</title><link>https://mind-benders.github.io/blog/p/competitive-programming-workshop/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/competitive-programming-workshop/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/competitive-programming-workshop/cpj.jpg" alt="Featured image of post Competitive Programming Workshop" />&lt;p>Get a head start in your career &amp;amp; learn to write efficient code. Mind Benders invite you to a hands-on workshop on Competitive Programming.&lt;/p>
&lt;p>What you will gain:&lt;/p>
&lt;ol>
&lt;li>Understand what is Competitive Programming (CP)&lt;/li>
&lt;li>Learn how to tackle the problems &amp;amp; code better&lt;/li>
&lt;li>Find out smart solutions for smart problems&lt;/li>
&lt;/ol>
&lt;p>The event will be conducted on 28th July, 2022 from 4:30 - 5:30 at Lab 308 @ TCET.&lt;/p>
&lt;p>Speaker : Jwala Chorasiya&lt;/p>
&lt;ul>
&lt;li>Intern/Educator at CodeChef&lt;/li>
&lt;li>4* CodeChef&lt;/li>
&lt;li>University Leader at Community Classroom&lt;/li>
&lt;/ul></description></item><item><title>Git &amp; Github Workshop</title><link>https://mind-benders.github.io/blog/p/git-github-workshop/</link><pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate><guid>https://mind-benders.github.io/blog/p/git-github-workshop/</guid><description>&lt;img src="https://mind-benders.github.io/blog/p/git-github-workshop/poster.jpeg" alt="Featured image of post Git &amp; Github Workshop" />&lt;p>Learn how to start building, shipping, and maintaining software with Git &amp;amp; GitHub. Mind Benders is pleased to announce that the workshop will be conducted you Dhiraj Chauhan from TE COMPS, who is a GitHub Campus Expert Trainee.&lt;/p>
&lt;p>The event will be conducted on 7th July, 22 from 4:30 - 5:30 at Lab 308 @ TCET.&lt;/p>
&lt;p>So Why Git?&lt;/p>
&lt;ul>
&lt;li>Over 70% of developers use Git!&lt;/li>
&lt;li>Developers can work together from anywhere in the world.&lt;/li>
&lt;li>Developers can see and revert to earlier versions of the project.&lt;/li>
&lt;/ul>
&lt;p>What will you learn:&lt;/p>
&lt;ol>
&lt;li>Git &amp;amp; Github&lt;/li>
&lt;li>Manage projects with Repositories&lt;/li>
&lt;li>Clone a project to work on a local copy&lt;/li>
&lt;li>Branch and Merge to work on different parts and versions of a project&lt;/li>
&lt;li>Push &amp;amp; Pull the latest version of the project to Github&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="post-event">Post Event&lt;/h2>
&lt;p>The event was a great success and had 20+ partcipants from the branch.&lt;/p>
&lt;p>Despite some minor hiccups the event was conducted smoothly with particpants taking home valuable lessons on why they should be using &amp;amp; implementing Git on their projects.&lt;/p>
&lt;p>More Fun &amp;amp; Interactive events are planned so Stay tuned.&lt;/p></description></item></channel></rss>